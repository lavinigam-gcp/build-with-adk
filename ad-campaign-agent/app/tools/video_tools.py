# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Video generation tools using two-stage pipeline.

Two-Stage Video Generation Pipeline:
    Stage 1: Scene Image (Gemini 2.0 Flash Exp)
        - Input: Product image + CreativeVariation parameters
        - Output: Scene-ready first frame (model wearing product)
        - Saved as: thumbnail

    Stage 2: Video Animation (Veo 3.1)
        - Input: Scene image + Animation prompt
        - Output: 8-second 9:16 video
        - Saved as: final video

HITL Workflow:
    - Videos are generated with status='generated'
    - NO metrics are created until activation
    - Use review_tools.activate_video() to push live
"""

import io
import json
import os
import time
from datetime import datetime
from typing import Optional, Dict, Any, Tuple

from PIL import Image as PILImage
from google import genai
from google.genai import types
from google.adk.tools import ToolContext

from ..config import (
    SELECTED_DIR,
    GENERATED_DIR,
    MODEL,
    IMAGE_GENERATION,
    VEO_MODEL,
    VIDEO_DURATION_SECONDS,
)
from ..database.db import get_db_cursor, get_product, get_product_by_name
from ..models.video_properties import VideoProperties
from ..models.variation import CreativeVariation, get_default_variation, PRESET_VARIATIONS
from .prompt_builders import build_scene_image_prompt, build_video_animation_prompt, build_creative_prompt
from .. import storage


def generate_video_prompt(metadata: dict, campaign_info: dict = None) -> str:
    """Generate a compelling video prompt from image metadata.

    Creates cinematic fashion video descriptions based on:
    - Model characteristics (from image analysis)
    - Clothing details (color, style, pattern)
    - Setting suggestions
    - Campaign context

    Args:
        metadata: Image analysis metadata dictionary
        campaign_info: Optional campaign context for additional styling

    Returns:
        Generated video prompt string
    """
    model_desc = metadata.get("model_description", "a model")
    clothing_desc = metadata.get("clothing_description", "elegant clothing")
    setting_desc = metadata.get("setting_description", "In a beautiful setting")
    garment_type = metadata.get("garment_type", "outfit")
    movement = metadata.get("movement", "moves gracefully")
    camera_style = metadata.get("camera_style", "slowly pans")
    key_feature = metadata.get("key_feature", "the details")
    mood = metadata.get("mood", "elegant, aspirational")

    prompt = f"""A cinematic fashion video featuring {model_desc} wearing {clothing_desc}. {setting_desc}, the {garment_type} {movement}. Camera {camera_style}, capturing {key_feature}. Atmosphere: {mood}. Professional lighting, high-end fashion advertisement style."""

    return prompt


async def analyze_video(video_path: str) -> VideoProperties:
    """Analyze a generated video using Gemini to extract structured properties.

    Uses VIDEO_ANALYSIS_MODEL from config with structured output (JSON Schema from Pydantic).
    This enables correlation between video properties and performance metrics.

    Args:
        video_path: Path to the video file (filename, relative path, or absolute path)

    Returns:
        VideoProperties: Structured properties extracted from the video
    """
    print(f"[DEBUG analyze_video] Analyzing video: {video_path}")
    print(f"[DEBUG analyze_video] Storage mode: {storage.get_storage_mode()}")

    # Extract just the filename for storage operations
    # Handle various input formats: gs:// URLs, absolute paths, relative paths, or bare filenames
    if video_path.startswith("gs://"):
        # GCS URL: gs://bucket/generated/filename.mp4 -> filename.mp4
        filename = video_path.split("/")[-1]
    elif os.path.isabs(video_path):
        # Absolute local path: /path/to/filename.mp4 -> filename.mp4
        filename = os.path.basename(video_path)
    elif video_path.startswith("generated/"):
        # Relative path: generated/filename.mp4 -> filename.mp4
        filename = video_path.replace("generated/", "")
    else:
        # Already a filename
        filename = video_path

    print(f"[DEBUG analyze_video] Extracted filename: {filename}")

    # Check if video exists using storage abstraction
    if not storage.video_exists(filename):
        print(f"[DEBUG analyze_video] Video file not found: {filename}")
        # Return default properties if file not found
        return VideoProperties()

    client = genai.Client()

    # Read video file using storage abstraction
    print(f"[DEBUG analyze_video] Reading video file...")
    video_bytes = storage.read_video(filename)
    print(f"[DEBUG analyze_video] Video size: {len(video_bytes)} bytes")

    # Create video part for Gemini
    video_part = types.Part.from_bytes(data=video_bytes, mime_type="video/mp4")

    prompt = """Analyze this fashion video advertisement and extract structured properties.

Focus on:
1. **Mood and emotional tone**: What feeling does the video evoke? (quirky, warm, bold, serene, mysterious, playful, sophisticated, energetic, elegant, romantic)
2. **Mood intensity**: How strong is this mood? (0.0 to 1.0)
3. **Warmth**: Does the video convey warmth/comfort? (true/false)
4. **Visual style**: What is the overall visual treatment? (cinematic, documentary, editorial, commercial, artistic, minimalist, vintage, modern)
5. **Camera movement**: Primary camera movement (static, pan, orbit, track, dolly, crane, handheld, slow_zoom)
6. **Lighting style**: Lighting approach (natural, studio, dramatic, soft, high_key, low_key, golden_hour, neon)
7. **Energy level**: Pace and movement intensity (calm, moderate, dynamic, high_energy)
8. **Movement amount**: Amount of subject movement (0.0 to 1.0)
9. **Color temperature**: Overall color grading (warm, neutral, cool)
10. **Dominant colors**: List the primary 2-4 colors in the video
11. **Color saturation**: How saturated are the colors? (0.0 to 1.0)
12. **Subject count**: Number of models/subjects visible
13. **Garment visibility**: How prominently is the garment featured? (0.0 to 1.0)
14. **Setting type**: Setting category (outdoor, studio, urban, nature, indoor, beach, etc.)
15. **Time of day**: Time depicted (golden_hour, day, night, dawn, dusk)
16. **Style tags**: List of 3-5 descriptive style tags
17. **Audio type**: Type of audio if present (none, ambient, music_upbeat, music_calm, music_dramatic, dialogue, voiceover)
18. **Background complexity**: How complex/busy is the background? (0.0 to 1.0)

Respond with a JSON object matching the VideoProperties schema. Be precise and consistent in your analysis."""

    try:
        print(f"[DEBUG analyze_video] Calling Gemini {MODEL} for video analysis...")
        response = client.models.generate_content(
            model=MODEL,
            contents=[video_part, prompt],
            config=types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=VideoProperties.model_json_schema()
            )
        )

        print(f"[DEBUG analyze_video] Response received")
        properties_dict = json.loads(response.text)
        print(f"[DEBUG analyze_video] Parsed properties: mood={properties_dict.get('mood')}, "
              f"energy={properties_dict.get('energy_level')}")

        return VideoProperties(**properties_dict)

    except Exception as e:
        print(f"[DEBUG analyze_video] Error analyzing video: {str(e)}")
        # Return default properties on error
        return VideoProperties()


# =============================================================================
# Two-Stage Video Generation Pipeline
# =============================================================================

async def generate_scene_image(
    product: Dict[str, Any],
    variation: CreativeVariation,
    product_image_bytes: bytes = None
) -> Tuple[bytes, str]:
    """Stage 1: Generate a scene-ready first frame image.

    Creates an image of a model wearing the product in the desired setting,
    which will be used as the first frame for video generation.

    Args:
        product: Product dictionary with metadata (from products table)
        variation: CreativeVariation parameters controlling the scene
        product_image_bytes: Optional product image bytes for reference

    Returns:
        Tuple of (scene_image_bytes, scene_prompt)
    """
    print(f"[DEBUG generate_scene_image] Starting scene generation for product: {product.get('name')}")
    print(f"[DEBUG generate_scene_image] Variation: {variation.name}")

    # Build scene prompt from product and variation
    scene_prompt = build_scene_image_prompt(product, variation)
    print(f"[DEBUG generate_scene_image] Scene prompt: {scene_prompt[:200]}...")

    client = genai.Client()

    # Use Gemini 2.0 Flash Exp for image generation (imagen-3.0-generate-002 alternative)
    # For now, using native image generation via Gemini
    try:
        contents = [scene_prompt]

        # If we have product image, include it as reference
        if product_image_bytes:
            print(f"[DEBUG generate_scene_image] Including product image as reference")
            image_part = types.Part.from_bytes(
                data=product_image_bytes,
                mime_type="image/png"
            )
            contents = [
                "Use this product image as reference for the garment. Generate a scene with a model wearing this exact garment:\n",
                image_part,
                "\n" + scene_prompt
            ]

        response = client.models.generate_content(
            model=IMAGE_GENERATION,
            contents=contents,
            config=types.GenerateContentConfig(
                response_modalities=["image", "text"]
            )
        )

        # Extract image from response
        scene_image_bytes = None
        for part in response.candidates[0].content.parts:
            if hasattr(part, 'inline_data') and part.inline_data:
                scene_image_bytes = part.inline_data.data
                print(f"[DEBUG generate_scene_image] Scene image generated: {len(scene_image_bytes)} bytes")
                break

        if not scene_image_bytes:
            raise ValueError("No image generated in response")

        return scene_image_bytes, scene_prompt

    except Exception as e:
        print(f"[DEBUG generate_scene_image] Error: {str(e)}")
        raise


async def animate_scene_with_veo(
    scene_image_bytes: bytes,
    product: Dict[str, Any],
    variation: CreativeVariation,
    duration_seconds: int = 8
) -> Tuple[bytes, str]:
    """Stage 2: Animate a scene image into a video using Veo 3.1.

    Args:
        scene_image_bytes: Scene image bytes from Stage 1
        product: Product dictionary with metadata
        variation: CreativeVariation parameters
        duration_seconds: Video duration (4, 6, or 8 seconds)

    Returns:
        Tuple of (video_bytes, video_prompt)
    """
    print(f"[DEBUG animate_scene_with_veo] Starting animation for: {product.get('name')}")
    print(f"[DEBUG animate_scene_with_veo] Duration: {duration_seconds}s")

    # Veo 3.1 only accepts duration of 4, 6, or 8 seconds
    valid_durations = [4, 6, 8]
    if duration_seconds not in valid_durations:
        duration_seconds = 8  # Default to 8 for best quality

    # Build animation-focused prompt
    video_prompt = build_video_animation_prompt(product, variation)
    print(f"[DEBUG animate_scene_with_veo] Animation prompt: {video_prompt[:200]}...")

    client = genai.Client()

    # Create image for Veo API
    image = types.Image(image_bytes=scene_image_bytes, mime_type="image/png")

    # Start video generation
    print(f"[DEBUG animate_scene_with_veo] Calling Veo ({VEO_MODEL})...")
    operation = client.models.generate_videos(
        model=VEO_MODEL,
        prompt=video_prompt,
        image=image,
        config=types.GenerateVideosConfig(
            number_of_videos=1,
            duration_seconds=duration_seconds,
        ),
    )

    # Poll for completion
    max_wait_time = 600  # 10 minutes
    poll_interval = 20
    waited = 0

    while not operation.done:
        if waited >= max_wait_time:
            raise TimeoutError(f"Video generation timed out after {max_wait_time} seconds")

        print(f"[DEBUG animate_scene_with_veo] Waiting... ({waited}s elapsed)")
        time.sleep(poll_interval)
        waited += poll_interval
        operation = client.operations.get(operation)

    print(f"[DEBUG animate_scene_with_veo] Operation completed after {waited}s")

    # Check result
    if operation.result is None or not operation.result.generated_videos:
        raise ValueError("Video generation completed but returned no result")

    # Extract video bytes
    generated_video = operation.result.generated_videos[0]

    is_vertex_ai = os.environ.get("GOOGLE_GENAI_USE_VERTEXAI", "").lower() == "true"

    if is_vertex_ai:
        video_bytes = generated_video.video.video_bytes
        if not video_bytes:
            raise ValueError("No video_bytes in Vertex AI response")
    else:
        # Gemini Developer API: Must download first
        client.files.download(file=generated_video.video)
        import tempfile
        with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp:
            temp_path = tmp.name
        generated_video.video.save(temp_path)
        with open(temp_path, "rb") as f:
            video_bytes = f.read()
        os.unlink(temp_path)

    print(f"[DEBUG animate_scene_with_veo] Video generated: {len(video_bytes)} bytes")
    return video_bytes, video_prompt


def generate_video_filename(product_name: str, variation_name: str) -> str:
    """Generate a descriptive video filename.

    Format: {product-name}-{MMDDYY}-{variation-name}.mp4

    Args:
        product_name: Product name (already hyphenated)
        variation_name: Variation name (already hyphenated)

    Returns:
        Video filename string
    """
    date_str = datetime.now().strftime("%m%d%y")
    return f"{product_name}-{date_str}-{variation_name}.mp4"


def save_video_metadata(
    video_filename: str,
    product: Dict[str, Any],
    variation: CreativeVariation,
    scene_prompt: str,
    video_prompt: str,
    pipeline_type: str = "two-stage"
) -> str:
    """Save video metadata alongside the video file.

    Creates a .txt file with the same name as the video containing
    all generation parameters for reproducibility.

    Args:
        video_filename: The video filename (without path)
        product: Product dictionary
        variation: CreativeVariation used
        scene_prompt: Stage 1 prompt
        video_prompt: Stage 2 prompt
        pipeline_type: 'two-stage' or 'single-stage'

    Returns:
        Path to the metadata file
    """
    metadata_filename = video_filename.replace(".mp4", ".txt")

    metadata_content = f"""Product: {product.get('name', 'unknown')}
Variation: {variation.name}
Pipeline: {pipeline_type.title()} {'(Scene + Animation)' if pipeline_type == 'two-stage' else ''}
Generated: {datetime.now().isoformat()}

Variation Parameters:
{json.dumps(variation.to_dict(), indent=2)}

STAGE 1 - Scene Image Prompt:
{scene_prompt}

STAGE 2 - Video Generation Prompt:
{video_prompt}
"""

    # Save metadata file
    if storage.get_storage_mode() == "gcs":
        # For GCS, save alongside video
        storage.save_video(metadata_filename, metadata_content.encode('utf-8'))
        metadata_path = storage.get_video_path(metadata_filename)
    else:
        metadata_path = os.path.join(GENERATED_DIR, metadata_filename)
        with open(metadata_path, "w") as f:
            f.write(metadata_content)

    print(f"[DEBUG save_video_metadata] Saved metadata to: {metadata_path}")
    return metadata_path


async def generate_video_from_product(
    campaign_id: int,
    product_id: int,
    variation: Optional[dict] = None,
    use_two_stage: bool = True,
    duration_seconds: int = 8,
    tool_context: ToolContext = None
) -> dict:
    """Generate a video ad using the two-stage pipeline.

    This is the primary video generation function that:
    1. Uses product from the products table
    2. Applies CreativeVariation parameters
    3. Generates via two-stage pipeline (scene image → video animation)
    4. Saves to campaign_videos table with status='generated'
    5. Does NOT create mock metrics (metrics only on activation)

    Args:
        campaign_id: The campaign to generate for
        product_id: The product ID from products table
        variation: Optional dict with variation parameters. Supported keys:
            - name: Unique variation identifier
            - model_ethnicity: asian, european, african, latina, south-asian, diverse
            - setting: studio, beach, urban, cafe, rooftop, garden, nature, etc.
            - mood: elegant, romantic, bold, playful, sophisticated, etc.
            - lighting: natural, studio, dramatic, soft, golden, neon, moody
            - activity: walking, standing, sitting, dancing, spinning, posing
            - camera_movement: orbit, pan, dolly, static, tracking, crane
            - time_of_day: golden-hour, sunrise, day, sunset, dusk, night
            - visual_style: cinematic, editorial, commercial, artistic
            - energy: calm, moderate, dynamic, high-energy
            If None, uses elegant studio defaults.
        use_two_stage: Use two-stage pipeline (default True)
        duration_seconds: Video duration (4, 6, or 8 seconds)
        tool_context: Optional ADK ToolContext for artifact storage

    Returns:
        Dictionary with video details and status='generated'
    """
    print(f"[DEBUG generate_video_from_product] Starting for campaign_id={campaign_id}, product_id={product_id}")

    # Ensure generated directory exists (only in local mode)
    if storage.get_storage_mode() == "local":
        os.makedirs(GENERATED_DIR, exist_ok=True)

    # Get campaign
    with get_db_cursor() as cursor:
        cursor.execute('SELECT * FROM campaigns WHERE id = ?', (campaign_id,))
        campaign = cursor.fetchone()
        if not campaign:
            return {"status": "error", "message": f"Campaign {campaign_id} not found"}

    # Get product from products table
    product = get_product(product_id)
    if not product:
        return {"status": "error", "message": f"Product {product_id} not found"}

    print(f"[DEBUG generate_video_from_product] Product: {product['name']}")
    print(f"[DEBUG generate_video_from_product] Campaign: {campaign['name']}")

    # Convert dict to CreativeVariation or use default
    # ADK 1.21+ requires dict instead of Pydantic model in function signature
    if variation is None:
        variation_obj = get_default_variation()
    elif isinstance(variation, dict):
        try:
            variation_obj = CreativeVariation.model_validate(variation)
        except Exception as e:
            print(f"[DEBUG generate_video_from_product] Variation validation error: {e}")
            # Fall back to default with any valid fields from dict
            variation_obj = get_default_variation()
            for key, value in variation.items():
                if hasattr(variation_obj, key):
                    setattr(variation_obj, key, value)
    elif isinstance(variation, CreativeVariation):
        # Already a CreativeVariation object (internal calls)
        variation_obj = variation
    else:
        variation_obj = get_default_variation()

    print(f"[DEBUG generate_video_from_product] Variation: {variation_obj.name}")

    # Check if product is linked to campaign
    with get_db_cursor() as cursor:
        cursor.execute('''
            SELECT id FROM campaign_products
            WHERE campaign_id = ? AND product_id = ?
        ''', (campaign_id, product_id))
        if not cursor.fetchone():
            # Auto-link product to campaign
            cursor.execute('''
                INSERT INTO campaign_products (campaign_id, product_id)
                VALUES (?, ?)
            ''', (campaign_id, product_id))
            print(f"[DEBUG generate_video_from_product] Linked product to campaign")

    # Get product image bytes using storage abstraction
    product_image_filename = product.get('image_filename')
    product_image_bytes = None
    if product_image_filename:
        try:
            if storage.product_image_exists(product_image_filename):
                product_image_bytes = storage.read_product_image(product_image_filename)
                image_path = storage.get_product_image_path(product_image_filename)
                print(f"[DEBUG generate_video_from_product] Loaded product image from: {image_path}")
            else:
                print(f"[DEBUG generate_video_from_product] Product image not found: {product_image_filename}")
        except Exception as e:
            print(f"[DEBUG generate_video_from_product] Could not load product image: {e}")

    # Generate video filename
    video_filename = generate_video_filename(product['name'], variation_obj.name)
    thumbnail_filename = video_filename.replace('.mp4', '-thumbnail.png')

    try:
        start_time = time.time()

        if use_two_stage:
            # Stage 1: Generate scene image
            print(f"[DEBUG generate_video_from_product] Stage 1: Generating scene image...")
            scene_image_bytes, scene_prompt = await generate_scene_image(
                product=product,
                variation=variation_obj,
                product_image_bytes=product_image_bytes
            )

            # Save scene image as thumbnail
            if storage.get_storage_mode() == "gcs":
                thumbnail_path = storage.save_video(thumbnail_filename, scene_image_bytes)
            else:
                thumbnail_path = os.path.join(GENERATED_DIR, thumbnail_filename)
                with open(thumbnail_path, 'wb') as f:
                    f.write(scene_image_bytes)
            print(f"[DEBUG generate_video_from_product] Saved thumbnail: {thumbnail_path}")

            # Stage 2: Animate scene with Veo
            print(f"[DEBUG generate_video_from_product] Stage 2: Animating with Veo 3.1...")
            video_bytes, video_prompt = await animate_scene_with_veo(
                scene_image_bytes=scene_image_bytes,
                product=product,
                variation=variation_obj,
                duration_seconds=duration_seconds
            )
        else:
            # Single-stage: Direct video generation (fallback)
            print(f"[DEBUG generate_video_from_product] Single-stage video generation...")
            scene_prompt = ""
            video_prompt = build_creative_prompt(product, variation_obj)

            # Load product image for direct Veo generation
            if not product_image_bytes:
                return {"status": "error", "message": "Product image required for single-stage generation"}

            image = types.Image(image_bytes=product_image_bytes, mime_type="image/png")

            client = genai.Client()
            operation = client.models.generate_videos(
                model=VEO_MODEL,
                prompt=video_prompt,
                image=image,
                config=types.GenerateVideosConfig(
                    number_of_videos=1,
                    duration_seconds=duration_seconds,
                ),
            )

            # Poll for completion
            max_wait_time = 600
            poll_interval = 20
            waited = 0
            while not operation.done:
                if waited >= max_wait_time:
                    raise TimeoutError("Video generation timed out")
                time.sleep(poll_interval)
                waited += poll_interval
                operation = client.operations.get(operation)

            if operation.result is None or not operation.result.generated_videos:
                raise ValueError("No video generated")

            generated_video = operation.result.generated_videos[0]
            is_vertex_ai = os.environ.get("GOOGLE_GENAI_USE_VERTEXAI", "").lower() == "true"

            if is_vertex_ai:
                video_bytes = generated_video.video.video_bytes
            else:
                client.files.download(file=generated_video.video)
                import tempfile
                with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp:
                    temp_path = tmp.name
                generated_video.video.save(temp_path)
                with open(temp_path, "rb") as f:
                    video_bytes = f.read()
                os.unlink(temp_path)

            thumbnail_path = None
            thumbnail_filename = None

        generation_time = int(time.time() - start_time)
        print(f"[DEBUG generate_video_from_product] Total generation time: {generation_time}s")

        # Save video
        if storage.get_storage_mode() == "gcs":
            video_path = storage.save_video(video_filename, video_bytes)
        else:
            video_path = os.path.join(GENERATED_DIR, video_filename)
            with open(video_path, 'wb') as f:
                f.write(video_bytes)
        print(f"[DEBUG generate_video_from_product] Saved video: {video_path}")

        # Save metadata file
        save_video_metadata(
            video_filename=video_filename,
            product=product,
            variation=variation_obj,
            scene_prompt=scene_prompt,
            video_prompt=video_prompt,
            pipeline_type="two-stage" if use_two_stage else "single-stage"
        )

        # Save as ADK artifact if tool_context provided
        if tool_context:
            video_artifact = types.Part.from_bytes(data=video_bytes, mime_type="video/mp4")
            version = await tool_context.save_artifact(filename=video_filename, artifact=video_artifact)
            print(f"[DEBUG generate_video_from_product] Saved artifact version: {version}")

        # Insert into campaign_videos table with status='generated'
        # NOTE: NO metrics are created - metrics only on activation
        with get_db_cursor() as cursor:
            cursor.execute('''
                INSERT INTO campaign_videos (
                    campaign_id, product_id, video_filename, local_path, thumbnail_path,
                    scene_prompt, video_prompt, pipeline_type, variation_name, variation_params,
                    duration_seconds, aspect_ratio, status, generation_time_seconds
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'generated', ?)
            ''', (
                campaign_id,
                product_id,
                video_filename,
                video_path,
                thumbnail_path,
                scene_prompt,
                video_prompt,
                "two-stage" if use_two_stage else "single-stage",
                variation_obj.name,
                json.dumps(variation_obj.to_dict()),
                duration_seconds,
                "9:16",
                generation_time
            ))
            video_id = cursor.lastrowid

        return {
            "status": "success",
            "message": f"Video generated successfully. Use activate_video to push live.",
            "video": {
                "id": video_id,
                "campaign_id": campaign_id,
                "campaign_name": campaign["name"],
                "product_id": product_id,
                "product_name": product["name"],
                "video_filename": video_filename,
                "video_path": video_path,
                "thumbnail_path": thumbnail_path,
                "variation": variation_obj.name,
                "pipeline": "two-stage" if use_two_stage else "single-stage",
                "duration_seconds": duration_seconds,
                "generation_time_seconds": generation_time,
                "status": "generated",
                "artifact_saved": tool_context is not None
            },
            "prompts": {
                "scene_prompt": scene_prompt[:200] + "..." if len(scene_prompt) > 200 else scene_prompt,
                "video_prompt": video_prompt[:200] + "..." if len(video_prompt) > 200 else video_prompt
            },
            "note": "Video is in 'generated' status. Metrics will only be created after activation."
        }

    except Exception as e:
        import traceback
        print(f"[DEBUG generate_video_from_product] Error: {str(e)}")
        print(f"[DEBUG generate_video_from_product] Traceback: {traceback.format_exc()}")
        return {
            "status": "error",
            "message": f"Video generation failed: {str(e)}",
            "product": product["name"],
            "variation": variation_obj.name if variation_obj else "default"
        }


def build_templated_prompt(
    base_metadata: dict,
    property_overrides: Optional[Dict[str, Any]] = None
) -> str:
    """Build a video prompt from templates with property overrides.

    This enables natural language control over video generation by allowing
    specific properties to be specified (mood, energy, style, etc.)

    Args:
        base_metadata: Image metadata (clothing, model description, etc.)
        property_overrides: Optional dict of properties to override defaults
            e.g., {"mood": "quirky", "energy_level": "high_energy", "color_temperature": "warm"}

    Returns:
        Generated prompt string
    """
    # Default properties
    props = {
        "mood": "elegant",
        "visual_style": "cinematic",
        "energy_level": "moderate",
        "color_temperature": "neutral",
        "camera_movement": "orbit",
        "lighting_style": "studio",
        "setting_type": "studio",
        "time_of_day": "day"
    }

    # Apply overrides
    if property_overrides:
        props.update(property_overrides)

    # Map mood to prompt fragments
    mood_map = {
        "quirky": "with a quirky, unexpected charm",
        "warm": "with warm, inviting atmosphere",
        "bold": "with bold, striking confidence",
        "serene": "with serene, peaceful elegance",
        "mysterious": "with an air of mystery and allure",
        "playful": "with playful, fun energy",
        "sophisticated": "with sophisticated refinement",
        "energetic": "with vibrant, high energy",
        "elegant": "with timeless elegance",
        "romantic": "with romantic, dreamy quality"
    }

    # Map energy to movement descriptions
    energy_map = {
        "calm": "moves gently and gracefully",
        "moderate": "moves with measured elegance",
        "dynamic": "moves with dynamic energy",
        "high_energy": "moves with vibrant, high energy"
    }

    # Map camera movement to descriptions
    camera_map = {
        "static": "Camera holds steady",
        "pan": "Camera pans smoothly",
        "orbit": "Camera slowly orbits around the subject",
        "track": "Camera tracks alongside the subject",
        "dolly": "Camera dollies in smoothly",
        "crane": "Camera moves with crane-like fluidity",
        "handheld": "Camera has subtle handheld movement",
        "slow_zoom": "Camera slowly zooms"
    }

    # Map visual style
    style_map = {
        "cinematic": "Cinematic",
        "documentary": "Documentary-style",
        "editorial": "High fashion editorial",
        "commercial": "Commercial",
        "artistic": "Artistic",
        "minimalist": "Minimalist",
        "vintage": "Vintage-inspired",
        "modern": "Modern contemporary"
    }

    # Build prompt components from metadata
    model_desc = base_metadata.get("model_description", "a model")
    clothing_desc = base_metadata.get("clothing_description", "elegant clothing")
    garment_type = base_metadata.get("garment_type", "outfit")

    # Get mapped values
    mood_text = mood_map.get(props["mood"], f"with {props['mood']} atmosphere")
    energy_text = energy_map.get(props["energy_level"], "moves gracefully")
    camera_text = camera_map.get(props["camera_movement"], f"Camera {props['camera_movement']}")
    style_text = style_map.get(props["visual_style"], props["visual_style"])

    # Build the templated prompt
    prompt = f"""{style_text} fashion video featuring {model_desc} wearing {clothing_desc}.
The subject {energy_text} {mood_text}.
{camera_text}, capturing the {garment_type} in {props['lighting_style']} lighting.
Color grading: {props['color_temperature']} tones.
Setting: {props['setting_type']} environment during {props['time_of_day']}.
Professional high-end fashion advertisement style."""

    return prompt


async def generate_video_ad(
    campaign_id: int,
    image_id: Optional[int] = None,
    custom_prompt: Optional[str] = None,
    duration_seconds: int = 6,
    tool_context: ToolContext = None
) -> dict:
    """Generate a video ad using Veo 3.1.

    Uses a seed image as reference and generates a cinematic fashion video.
    Polls for completion and saves to the generated/ folder.
    Optionally saves video as ADK artifact if tool_context is provided.

    Args:
        campaign_id: The campaign to generate the ad for
        image_id: Optional specific image ID to use. If not provided, uses the first image.
        custom_prompt: Optional custom prompt. If not provided, generates from image metadata.
        duration_seconds: Video duration (4, 6, or 8 seconds for Veo 3.1)
        tool_context: Optional ADK ToolContext for artifact storage

    Returns:
        Dictionary with video path and generation details
    """
    print(f"[DEBUG generate_video_ad] Starting for campaign_id={campaign_id}")
    print(f"[DEBUG generate_video_ad] image_id={image_id}, duration_seconds={duration_seconds}")
    print(f"[DEBUG generate_video_ad] custom_prompt={custom_prompt[:100] if custom_prompt else 'None'}...")

    # Veo 3.1 only accepts duration of 4, 6, or 8 seconds
    valid_durations = [4, 6, 8]
    if duration_seconds not in valid_durations:
        # Map to nearest valid duration
        if duration_seconds <= 4:
            duration_seconds = 4
        elif duration_seconds <= 6:
            duration_seconds = 6
        else:
            duration_seconds = 8
        print(f"[DEBUG generate_video_ad] Adjusted duration to: {duration_seconds}")

    # Ensure generated directory exists (only in local mode)
    if storage.get_storage_mode() == "local":
        os.makedirs(GENERATED_DIR, exist_ok=True)
        print(f"[DEBUG generate_video_ad] GENERATED_DIR: {GENERATED_DIR}")
    else:
        print(f"[DEBUG generate_video_ad] Using GCS storage, skipping local directory creation")

    with get_db_cursor() as cursor:
        # Get campaign info
        print(f"[DEBUG generate_video_ad] Fetching campaign {campaign_id} from database...")
        cursor.execute('SELECT * FROM campaigns WHERE id = ?', (campaign_id,))
        campaign = cursor.fetchone()
        if not campaign:
            print(f"[DEBUG generate_video_ad] Campaign {campaign_id} not found")
            return {
                "status": "error",
                "message": f"Campaign with ID {campaign_id} not found"
            }
        print(f"[DEBUG generate_video_ad] Found campaign: {campaign['name']}")

        # Get image
        if image_id:
            print(f"[DEBUG generate_video_ad] Fetching specific image {image_id}...")
            cursor.execute('''
                SELECT * FROM campaign_images
                WHERE id = ? AND campaign_id = ?
            ''', (image_id, campaign_id))
        else:
            print(f"[DEBUG generate_video_ad] Fetching first image for campaign...")
            cursor.execute('''
                SELECT * FROM campaign_images
                WHERE campaign_id = ?
                ORDER BY created_at
                LIMIT 1
            ''', (campaign_id,))

        image_row = cursor.fetchone()
        if not image_row:
            print(f"[DEBUG generate_video_ad] No images found for campaign {campaign_id}")
            return {
                "status": "error",
                "message": f"No images found for campaign {campaign_id}. Add a seed image first."
            }
        print(f"[DEBUG generate_video_ad] Found image: {image_row['image_path']}")

        image_filename = image_row["image_path"]
        image_path = storage.get_image_path(image_filename)
        print(f"[DEBUG generate_video_ad] Image path: {image_path}")
        print(f"[DEBUG generate_video_ad] Storage mode: {storage.get_storage_mode()}")
        if not storage.image_exists(image_filename):
            print(f"[DEBUG generate_video_ad] Image file not found: {image_filename}")
            return {
                "status": "error",
                "message": f"Image file not found: {image_filename}"
            }
        print(f"[DEBUG generate_video_ad] Image file exists")

        # Get or generate prompt
        if custom_prompt:
            prompt = custom_prompt
            print(f"[DEBUG generate_video_ad] Using custom prompt")
        else:
            metadata = json.loads(image_row["metadata"]) if image_row["metadata"] else {}
            campaign_info = {
                "name": campaign["name"],
                "category": campaign["category"],
                "city": campaign["city"],
                "state": campaign["state"]
            }
            prompt = generate_video_prompt(metadata, campaign_info)
            print(f"[DEBUG generate_video_ad] Generated prompt from metadata")
        print(f"[DEBUG generate_video_ad] Prompt: {prompt[:100]}...")

        # Create pending ad record
        print(f"[DEBUG generate_video_ad] Creating pending ad record...")
        cursor.execute('''
            INSERT INTO campaign_ads (campaign_id, image_id, video_path, prompt_used, duration_seconds, status)
            VALUES (?, ?, '', ?, ?, 'generating')
        ''', (campaign_id, image_row["id"], prompt, duration_seconds))
        ad_id = cursor.lastrowid
        print(f"[DEBUG generate_video_ad] Created ad record with id={ad_id}")

    # Generate video using Veo 3.1
    try:
        print(f"[DEBUG generate_video_ad] Initializing genai client...")
        client = genai.Client()

        # Load image using storage abstraction and convert to bytes for Veo API
        # This follows the official Veo documentation pattern
        print(f"[DEBUG generate_video_ad] Loading image...")
        image_bytes = storage.read_image(image_filename)
        print(f"[DEBUG generate_video_ad] Image bytes size: {len(image_bytes)}")

        # Use PIL to determine format and re-encode if needed
        with PILImage.open(io.BytesIO(image_bytes)) as im:
            print(f"[DEBUG generate_video_ad] Image format: {im.format}, size: {im.size}")
            img_format = im.format or "JPEG"

        # Create types.Image for Veo API (NOT types.Part)
        mime_type = f"image/{img_format.lower()}"
        print(f"[DEBUG generate_video_ad] Creating types.Image with mime_type={mime_type}")
        image = types.Image(image_bytes=image_bytes, mime_type=mime_type)

        # Start video generation
        print(f"[DEBUG generate_video_ad] Starting video generation with {VEO_MODEL}...")
        operation = client.models.generate_videos(
            model=VEO_MODEL,
            prompt=prompt,
            image=image,
            config=types.GenerateVideosConfig(
                number_of_videos=1,
                duration_seconds=duration_seconds,
                # Note: enhance_prompt is NOT supported by veo-3.1-generate-preview
            ),
        )
        print(f"[DEBUG generate_video_ad] Video generation started, operation: {operation}")

        # Poll for completion (20 second intervals per official docs)
        max_wait_time = 600  # 10 minutes max for video generation
        poll_interval = 20  # 20 seconds per official docs
        waited = 0

        while not operation.done:
            if waited >= max_wait_time:
                print(f"[DEBUG generate_video_ad] Timed out after {max_wait_time} seconds")
                with get_db_cursor() as cursor:
                    cursor.execute('''
                        UPDATE campaign_ads SET status = 'failed' WHERE id = ?
                    ''', (ad_id,))
                return {
                    "status": "error",
                    "message": "Video generation timed out after 10 minutes",
                    "ad_id": ad_id
                }

            print(f"[DEBUG generate_video_ad] Waiting... ({waited}s elapsed)")
            time.sleep(poll_interval)
            waited += poll_interval
            operation = client.operations.get(operation)
            print(f"[DEBUG generate_video_ad] Operation done: {operation.done}")

        print(f"[DEBUG generate_video_ad] Operation completed after {waited}s")

        # Check if operation succeeded (use .result NOT .response per official docs)
        print(f"[DEBUG generate_video_ad] Checking result: {operation.result}")
        if operation.result is None or not operation.result.generated_videos:
            print(f"[DEBUG generate_video_ad] No result or no generated videos")
            with get_db_cursor() as cursor:
                cursor.execute('''
                    UPDATE campaign_ads SET status = 'failed' WHERE id = ?
                ''', (ad_id,))
            return {
                "status": "error",
                "message": "Video generation completed but returned no result. Check API quota and permissions.",
                "ad_id": ad_id,
                "prompt_used": prompt
            }

        print(f"[DEBUG generate_video_ad] Found {len(operation.result.generated_videos)} generated video(s)")

        # Get the generated video
        generated_video = operation.result.generated_videos[0]
        timestamp = int(time.time())
        output_filename = f"campaign_{campaign_id}_ad_{ad_id}_{timestamp}.mp4"

        # Handle video bytes differently for Vertex AI vs Gemini Developer API
        # - Vertex AI: video_bytes are already in the response (no download needed)
        # - Gemini Developer API: Must call client.files.download() to populate video_bytes
        is_vertex_ai = os.environ.get("GOOGLE_GENAI_USE_VERTEXAI", "").lower() == "true"
        print(f"[DEBUG generate_video_ad] Using Vertex AI: {is_vertex_ai}")

        if is_vertex_ai:
            # Vertex AI: video_bytes already present in response
            print(f"[DEBUG generate_video_ad] Vertex AI mode - using video_bytes from response")
            video_data = generated_video.video.video_bytes
            if not video_data:
                raise ValueError("No video_bytes in Vertex AI response")
            print(f"[DEBUG generate_video_ad] Video bytes size: {len(video_data)}")
        else:
            # Gemini Developer API: Must download first, then use .save()
            print(f"[DEBUG generate_video_ad] Gemini Developer API mode - downloading video...")
            client.files.download(file=generated_video.video)
            # For Gemini API, we need to save to temp file to get bytes
            import tempfile
            with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp:
                temp_path = tmp.name
            generated_video.video.save(temp_path)
            with open(temp_path, "rb") as f:
                video_data = f.read()
            os.unlink(temp_path)
            print(f"[DEBUG generate_video_ad] Video bytes size: {len(video_data)}")

        # Save video - handle both local and GCS storage modes
        if storage.get_storage_mode() == "gcs":
            output_path = storage.save_video(output_filename, video_data)
            print(f"[DEBUG generate_video_ad] Video uploaded to GCS: {output_path}")
        else:
            output_path = os.path.join(GENERATED_DIR, output_filename)
            print(f"[DEBUG generate_video_ad] Saving video to: {output_path}")
            with open(output_path, "wb") as f:
                f.write(video_data)
            print(f"[DEBUG generate_video_ad] Video saved successfully")

        # Save as ADK artifact if tool_context is provided
        if tool_context:
            print(f"[DEBUG generate_video_ad] Saving as ADK artifact...")
            # Use video_data we already have in memory (no need to re-read from storage)
            video_artifact = types.Part.from_bytes(data=video_data, mime_type="video/mp4")
            # save_artifact is async, await it properly
            version = await tool_context.save_artifact(filename=output_filename, artifact=video_artifact)
            print(f"[DEBUG generate_video_ad] Artifact saved, version: {version}")
        else:
            print(f"[DEBUG generate_video_ad] No tool_context, skipping artifact save")

        # Analyze the generated video to extract properties
        print(f"[DEBUG generate_video_ad] Analyzing generated video for properties...")
        video_properties = await analyze_video(output_path)
        print(f"[DEBUG generate_video_ad] Extracted properties: mood={video_properties.mood}, "
              f"energy={video_properties.energy_level}, style={video_properties.visual_style}")

        # Update database with video path and properties
        with get_db_cursor() as cursor:
            cursor.execute('''
                UPDATE campaign_ads
                SET video_path = ?, status = 'completed', video_properties = ?
                WHERE id = ?
            ''', (output_filename, video_properties.model_dump_json(), ad_id))

        # NOTE: Auto-metrics generation REMOVED per HITL workflow
        # Metrics are now only created when a video is activated via review_tools.activate_video()
        print(f"[DEBUG generate_video_ad] Video saved. No auto-metrics (HITL workflow).")

        return {
            "status": "success",
            "message": "Video ad generated successfully. Use activate_video to push live and generate metrics.",
            "ad": {
                "id": ad_id,
                "campaign_id": campaign_id,
                "campaign_name": campaign["name"],
                "video_path": output_filename,
                "full_path": output_path,
                "prompt_used": prompt,
                "duration_seconds": duration_seconds,
                "source_image": image_row["image_path"],
                "artifact_saved": tool_context is not None,
                "status": "completed"
            },
            "video_properties": video_properties.model_dump(),
            "note": "Metrics will only be generated after video activation (HITL workflow)."
        }

    except Exception as e:
        import traceback
        print(f"[DEBUG generate_video_ad] Exception: {str(e)}")
        print(f"[DEBUG generate_video_ad] Traceback: {traceback.format_exc()}")
        # Update ad status to failed
        with get_db_cursor() as cursor:
            cursor.execute('''
                UPDATE campaign_ads SET status = 'failed' WHERE id = ?
            ''', (ad_id,))

        return {
            "status": "error",
            "message": f"Video generation failed: {str(e)}",
            "ad_id": ad_id,
            "prompt_used": prompt
        }


def generate_video_variation(
    ad_id: int,
    variation_type: str = "setting"
) -> dict:
    """Generate a variation of an existing successful ad.

    Modifies the prompt based on variation_type to create an A/B testing variant.

    Args:
        ad_id: The ID of the original ad to create a variation of
        variation_type: Type of variation - one of: setting, mood, angle, style

    Returns:
        Dictionary with new video details
    """
    variation_modifiers = {
        "setting": [
            "In a luxurious urban rooftop at golden hour",
            "In an elegant minimalist studio with soft natural light",
            "On a scenic coastal backdrop with ocean breeze",
            "In a chic European café setting"
        ],
        "mood": [
            "Atmosphere: bold, confident, powerful",
            "Atmosphere: serene, peaceful, calming",
            "Atmosphere: energetic, vibrant, youthful",
            "Atmosphere: mysterious, alluring, sophisticated"
        ],
        "angle": [
            "Camera dramatically sweeps from low angle upward",
            "Camera follows in slow-motion tracking shot",
            "Camera circles in an elegant 360-degree orbit",
            "Camera captures from artistic bird's eye perspective"
        ],
        "style": [
            "Film noir style with dramatic shadows and contrast",
            "Soft focus romantic style with dreamy lens flare",
            "High contrast editorial style with sharp details",
            "Vintage film aesthetic with warm color grading"
        ]
    }

    if variation_type not in variation_modifiers:
        return {
            "status": "error",
            "message": f"Invalid variation_type. Must be one of: {', '.join(variation_modifiers.keys())}"
        }

    with get_db_cursor() as cursor:
        # Get original ad
        cursor.execute('''
            SELECT ca.*, c.name as campaign_name, ci.image_path, ci.metadata
            FROM campaign_ads ca
            JOIN campaigns c ON ca.campaign_id = c.id
            LEFT JOIN campaign_images ci ON ca.image_id = ci.id
            WHERE ca.id = ?
        ''', (ad_id,))

        original_ad = cursor.fetchone()
        if not original_ad:
            return {
                "status": "error",
                "message": f"Ad with ID {ad_id} not found"
            }

        if original_ad["status"] != "completed":
            return {
                "status": "error",
                "message": f"Original ad is not completed (status: {original_ad['status']})"
            }

    # Get a random modifier for the variation type
    import random
    modifier = random.choice(variation_modifiers[variation_type])

    # Modify the original prompt
    original_prompt = original_ad["prompt_used"]

    if variation_type == "setting":
        # Replace setting description
        parts = original_prompt.split(".")
        if len(parts) > 1:
            parts[1] = f" {modifier}"
        variation_prompt = ".".join(parts)
    elif variation_type == "mood":
        # Replace mood at the end
        if "Atmosphere:" in original_prompt:
            variation_prompt = original_prompt.rsplit("Atmosphere:", 1)[0] + modifier
        else:
            variation_prompt = original_prompt + " " + modifier
    elif variation_type == "angle":
        # Replace camera instruction
        if "Camera " in original_prompt:
            parts = original_prompt.split("Camera ")
            variation_prompt = parts[0] + modifier
            if len(parts) > 1 and "." in parts[1]:
                remaining = parts[1].split(".", 1)[1]
                variation_prompt += "." + remaining
        else:
            variation_prompt = original_prompt + " " + modifier
    else:  # style
        variation_prompt = original_prompt + " " + modifier

    # Generate the variation
    return generate_video_ad(
        campaign_id=original_ad["campaign_id"],
        image_id=original_ad["image_id"],
        custom_prompt=variation_prompt,
        duration_seconds=original_ad["duration_seconds"]
    )


async def apply_winning_formula(
    target_campaign_id: int,
    source_ad_id: int = None,
    characteristics_to_apply: list = None,
    target_image_id: int = None,
    tool_context: ToolContext = None
) -> dict:
    """Apply successful characteristics from a top-performing ad to generate new content.

    This tool bridges insights and action: after identifying what makes top performers
    successful, use this to apply those winning characteristics to other campaigns.

    Args:
        target_campaign_id: The campaign to create a new video ad for
        source_ad_id: The ad to learn from. If None, automatically uses the top performer.
        characteristics_to_apply: List of characteristics to preserve from source.
            Options: ["mood", "setting", "camera_style", "movement"]
            If None, applies all available characteristics.
        target_image_id: Specific image in target campaign to use. If None, uses first available.
        tool_context: ADK ToolContext for artifact storage

    Returns:
        Dictionary with new video details and applied characteristics

    Example:
        # After seeing insights that ad #1 has winning "dreamy, romantic" mood:
        apply_winning_formula(
            target_campaign_id=3,  # Urban Professional campaign
            source_ad_id=1,        # Top performer
            characteristics_to_apply=["mood", "setting"]
        )
    """
    print(f"[DEBUG apply_winning_formula] Starting...")
    print(f"[DEBUG apply_winning_formula] target_campaign_id={target_campaign_id}")
    print(f"[DEBUG apply_winning_formula] source_ad_id={source_ad_id}")
    print(f"[DEBUG apply_winning_formula] characteristics_to_apply={characteristics_to_apply}")

    # Extended characteristics when video_properties are available
    valid_characteristics = [
        "mood", "setting", "camera_style", "movement", "key_feature",
        # New video property characteristics
        "visual_style", "energy_level", "color_temperature", "lighting_style"
    ]

    if characteristics_to_apply:
        invalid = [c for c in characteristics_to_apply if c not in valid_characteristics]
        if invalid:
            return {
                "status": "error",
                "message": f"Invalid characteristics: {invalid}. Valid options: {valid_characteristics}"
            }

    with get_db_cursor() as cursor:
        # Step 1: Get source ad (top performer or specified) including video_properties
        if source_ad_id:
            print(f"[DEBUG apply_winning_formula] Using specified source_ad_id={source_ad_id}")
            cursor.execute('''
                SELECT ca.*, ca.video_properties, ci.metadata, ci.image_path as source_image,
                       c.name as campaign_name,
                       SUM(cm.revenue) as total_revenue
                FROM campaign_ads ca
                JOIN campaigns c ON ca.campaign_id = c.id
                LEFT JOIN campaign_images ci ON ca.image_id = ci.id
                LEFT JOIN campaign_metrics cm ON ca.id = cm.ad_id
                WHERE ca.id = ? AND ca.status = 'completed'
                GROUP BY ca.id
            ''', (source_ad_id,))
        else:
            print(f"[DEBUG apply_winning_formula] Auto-selecting top performer by revenue...")
            cursor.execute('''
                SELECT ca.*, ca.video_properties, ci.metadata, ci.image_path as source_image,
                       c.name as campaign_name,
                       SUM(cm.revenue) as total_revenue
                FROM campaign_ads ca
                JOIN campaigns c ON ca.campaign_id = c.id
                LEFT JOIN campaign_images ci ON ca.image_id = ci.id
                LEFT JOIN campaign_metrics cm ON ca.id = cm.ad_id
                WHERE ca.status = 'completed'
                GROUP BY ca.id
                ORDER BY total_revenue DESC
                LIMIT 1
            ''')

        source_ad = cursor.fetchone()
        if not source_ad:
            return {
                "status": "error",
                "message": "No completed ads found to learn from" if not source_ad_id
                          else f"Source ad {source_ad_id} not found or not completed"
            }

        print(f"[DEBUG apply_winning_formula] Source ad: id={source_ad['id']}, "
              f"campaign='{source_ad['campaign_name']}', revenue=${source_ad['total_revenue'] or 0:,.2f}")

        # Step 2: Extract winning characteristics from video_properties (preferred) or image metadata (fallback)
        source_video_props = None
        if source_ad["video_properties"]:
            try:
                source_video_props = json.loads(source_ad["video_properties"])
                print(f"[DEBUG apply_winning_formula] Using VIDEO PROPERTIES from source ad")
            except json.JSONDecodeError:
                pass

        source_metadata = json.loads(source_ad["metadata"]) if source_ad["metadata"] else {}

        # Build winning formula - prefer video_properties when available
        if source_video_props:
            winning_formula = {
                # From video_properties
                "mood": source_video_props.get("mood", "elegant"),
                "visual_style": source_video_props.get("visual_style", "cinematic"),
                "energy_level": source_video_props.get("energy_level", "moderate"),
                "color_temperature": source_video_props.get("color_temperature", "neutral"),
                "camera_style": source_video_props.get("camera_movement", "orbit"),
                "lighting_style": source_video_props.get("lighting_style", "studio"),
                "setting": source_video_props.get("setting_type", "studio"),
                # From image metadata (not in video_properties)
                "movement": source_metadata.get("movement", "moves gracefully"),
                "key_feature": source_metadata.get("key_feature", "the details"),
                "model_description": source_metadata.get("model_description", "a model"),
            }
            print(f"[DEBUG apply_winning_formula] Winning formula from VIDEO PROPERTIES:")
        else:
            # Fallback to image metadata only
            winning_formula = {
                "mood": source_metadata.get("mood", "elegant, aspirational"),
                "setting": source_metadata.get("setting_description", "beautiful setting"),
                "camera_style": source_metadata.get("camera_style", "smoothly captures"),
                "movement": source_metadata.get("movement", "moves gracefully"),
                "key_feature": source_metadata.get("key_feature", "the details"),
                "model_description": source_metadata.get("model_description", "a model"),
                # Defaults for new properties
                "visual_style": "cinematic",
                "energy_level": "moderate",
                "color_temperature": "neutral",
                "lighting_style": "studio",
            }
            print(f"[DEBUG apply_winning_formula] Winning formula from IMAGE METADATA (fallback):")

        print(f"[DEBUG apply_winning_formula] Winning formula extracted:")
        for k, v in winning_formula.items():
            print(f"[DEBUG apply_winning_formula]   - {k}: {v}")

        # Step 3: Get target campaign and image
        cursor.execute('SELECT * FROM campaigns WHERE id = ?', (target_campaign_id,))
        target_campaign = cursor.fetchone()
        if not target_campaign:
            return {
                "status": "error",
                "message": f"Target campaign {target_campaign_id} not found"
            }

        print(f"[DEBUG apply_winning_formula] Target campaign: '{target_campaign['name']}'")

        # Get target image
        if target_image_id:
            cursor.execute('''
                SELECT * FROM campaign_images
                WHERE id = ? AND campaign_id = ?
            ''', (target_image_id, target_campaign_id))
        else:
            cursor.execute('''
                SELECT * FROM campaign_images
                WHERE campaign_id = ?
                ORDER BY created_at
                LIMIT 1
            ''', (target_campaign_id,))

        target_image = cursor.fetchone()
        if not target_image:
            return {
                "status": "error",
                "message": f"No images found for target campaign {target_campaign_id}. Add a seed image first."
            }

        print(f"[DEBUG apply_winning_formula] Target image: {target_image['image_path']}")

        # Get target image metadata for clothing description
        target_metadata = json.loads(target_image["metadata"]) if target_image["metadata"] else {}

    # Step 4: Build prompt that PRESERVES winning characteristics
    # Use target image's clothing/garment but source ad's mood, setting, etc.

    # Determine which characteristics to apply - include new video properties if available
    if characteristics_to_apply:
        chars_to_use = characteristics_to_apply
    elif source_video_props:
        # When video_properties are available, apply more characteristics by default
        chars_to_use = ["mood", "visual_style", "energy_level", "color_temperature", "setting", "camera_style", "lighting_style"]
    else:
        chars_to_use = ["mood", "setting", "camera_style", "movement"]

    # Build the prompt components
    model_desc = target_metadata.get("model_description", winning_formula["model_description"])
    clothing_desc = target_metadata.get("clothing_description", "elegant clothing")
    garment_type = target_metadata.get("garment_type", "outfit")

    # Apply winning characteristics
    if "setting" in chars_to_use:
        setting_desc = winning_formula["setting"]
    else:
        setting_desc = target_metadata.get("setting_description", "In a beautiful setting")

    if "mood" in chars_to_use:
        mood = winning_formula["mood"]
    else:
        mood = target_metadata.get("mood", "elegant, aspirational")

    if "camera_style" in chars_to_use:
        camera_style = winning_formula["camera_style"]
    else:
        camera_style = target_metadata.get("camera_style", "slowly pans")

    if "movement" in chars_to_use:
        movement = winning_formula["movement"]
    else:
        movement = target_metadata.get("movement", "moves gracefully")

    if "key_feature" in chars_to_use:
        key_feature = winning_formula["key_feature"]
    else:
        key_feature = target_metadata.get("key_feature", "the details")

    # New video property characteristics
    visual_style = winning_formula.get("visual_style", "cinematic") if "visual_style" in chars_to_use else "cinematic"
    energy_level = winning_formula.get("energy_level", "moderate") if "energy_level" in chars_to_use else "moderate"
    color_temperature = winning_formula.get("color_temperature", "neutral") if "color_temperature" in chars_to_use else "neutral"
    lighting_style = winning_formula.get("lighting_style", "studio") if "lighting_style" in chars_to_use else "professional"

    # Use templated prompt builder if video properties are available
    if source_video_props:
        # Build property overrides for templated prompt
        property_overrides = {
            "mood": mood,
            "visual_style": visual_style,
            "energy_level": energy_level,
            "color_temperature": color_temperature,
            "camera_movement": camera_style,
            "lighting_style": lighting_style,
            "setting_type": setting_desc,
        }
        winning_prompt = build_templated_prompt(target_metadata, property_overrides)
    else:
        # Fallback to original prompt format
        winning_prompt = f"""A cinematic fashion video featuring {model_desc} wearing {clothing_desc}. {setting_desc}, the {garment_type} {movement}. Camera {camera_style}, capturing {key_feature}. Atmosphere: {mood}. Professional lighting, high-end fashion advertisement style."""

    print(f"[DEBUG apply_winning_formula] Generated prompt with winning formula:")
    print(f"[DEBUG apply_winning_formula] {winning_prompt[:200]}...")
    print(f"[DEBUG apply_winning_formula] Applied characteristics: {chars_to_use}")
    print(f"[DEBUG apply_winning_formula] Using video_properties format: {source_video_props is not None}")

    # Step 5: Generate the video using the winning formula
    result = await generate_video_ad(
        campaign_id=target_campaign_id,
        image_id=target_image["id"],
        custom_prompt=winning_prompt,
        duration_seconds=6,
        tool_context=tool_context
    )

    # Enhance the result with winning formula details
    if result["status"] == "success":
        result["winning_formula_applied"] = {
            "source_ad_id": source_ad["id"],
            "source_campaign": source_ad["campaign_name"],
            "source_revenue": round(source_ad["total_revenue"], 2) if source_ad["total_revenue"] else 0,
            "characteristics_applied": chars_to_use,
            "formula": {k: winning_formula[k] for k in chars_to_use if k in winning_formula}
        }
        result["message"] = f"Video generated using winning formula from top performer (Ad #{source_ad['id']})"

    return result


def list_campaign_ads(campaign_id: int) -> dict:
    """List all generated video ads for a campaign.

    Args:
        campaign_id: The ID of the campaign

    Returns:
        Dictionary with list of ads and their details
    """
    with get_db_cursor() as cursor:
        cursor.execute('SELECT id, name FROM campaigns WHERE id = ?', (campaign_id,))
        campaign = cursor.fetchone()
        if not campaign:
            return {
                "status": "error",
                "message": f"Campaign with ID {campaign_id} not found"
            }

        cursor.execute('''
            SELECT ca.*, ci.image_path as source_image
            FROM campaign_ads ca
            LEFT JOIN campaign_images ci ON ca.image_id = ci.id
            WHERE ca.campaign_id = ?
            ORDER BY ca.created_at DESC
        ''', (campaign_id,))

        ads = []
        for row in cursor.fetchall():
            video_filename = row["video_path"]
            # Use storage abstraction for path resolution and existence check
            video_path = storage.get_video_path(video_filename) if video_filename else None
            video_exists = storage.video_exists(video_filename) if video_filename else False
            # Parse video properties if available
            video_props = None
            if row["video_properties"]:
                try:
                    video_props = json.loads(row["video_properties"])
                except json.JSONDecodeError:
                    pass
            ads.append({
                "id": row["id"],
                "video_path": video_filename,
                "full_path": video_path,
                "exists": video_exists,
                "prompt_used": row["prompt_used"],
                "duration_seconds": row["duration_seconds"],
                "status": row["status"],
                "source_image": row["source_image"],
                "video_properties": video_props,
                "created_at": row["created_at"]
            })

        return {
            "status": "success",
            "campaign_id": campaign_id,
            "campaign_name": campaign["name"],
            "ad_count": len(ads),
            "ads": ads
        }


async def generate_video_with_properties(
    campaign_id: int,
    image_id: Optional[int] = None,
    mood: Optional[str] = None,
    visual_style: Optional[str] = None,
    energy_level: Optional[str] = None,
    color_temperature: Optional[str] = None,
    camera_movement: Optional[str] = None,
    lighting_style: Optional[str] = None,
    setting_type: Optional[str] = None,
    time_of_day: Optional[str] = None,
    duration_seconds: int = 6,
    tool_context: ToolContext = None
) -> dict:
    """Generate a video ad with specific property controls.

    This enables natural language variation requests like:
    "Generate a quirky, high-energy video with warm colors"

    All property parameters are optional - only specify the ones you want to control.
    Unspecified properties will use sensible defaults.

    Args:
        campaign_id: The campaign to generate for
        image_id: Optional specific image ID to use
        mood: Mood override (quirky, warm, bold, serene, mysterious, playful, sophisticated, energetic, elegant, romantic)
        visual_style: Style override (cinematic, documentary, editorial, commercial, artistic, minimalist, vintage, modern)
        energy_level: Energy override (calm, moderate, dynamic, high_energy)
        color_temperature: Color override (warm, neutral, cool)
        camera_movement: Camera override (static, pan, orbit, track, dolly, crane, handheld, slow_zoom)
        lighting_style: Lighting override (natural, studio, dramatic, soft, high_key, low_key, golden_hour, neon)
        setting_type: Setting override (outdoor, studio, urban, nature, indoor, beach, etc.)
        time_of_day: Time override (golden_hour, day, night, dawn, dusk)
        duration_seconds: Video duration (4, 6, or 8 seconds)
        tool_context: ADK ToolContext for artifact storage

    Returns:
        Dictionary with video details and extracted properties
    """
    print(f"[DEBUG generate_video_with_properties] Starting for campaign_id={campaign_id}")

    # Build property overrides from parameters
    property_overrides = {}
    if mood:
        property_overrides["mood"] = mood
    if visual_style:
        property_overrides["visual_style"] = visual_style
    if energy_level:
        property_overrides["energy_level"] = energy_level
    if color_temperature:
        property_overrides["color_temperature"] = color_temperature
    if camera_movement:
        property_overrides["camera_movement"] = camera_movement
    if lighting_style:
        property_overrides["lighting_style"] = lighting_style
    if setting_type:
        property_overrides["setting_type"] = setting_type
    if time_of_day:
        property_overrides["time_of_day"] = time_of_day

    print(f"[DEBUG generate_video_with_properties] Property overrides: {property_overrides}")

    # Get image metadata
    with get_db_cursor() as cursor:
        if image_id:
            cursor.execute('''
                SELECT metadata FROM campaign_images
                WHERE campaign_id = ? AND id = ?
            ''', (campaign_id, image_id))
        else:
            cursor.execute('''
                SELECT metadata FROM campaign_images
                WHERE campaign_id = ?
                ORDER BY created_at
                LIMIT 1
            ''', (campaign_id,))

        row = cursor.fetchone()
        if not row:
            return {
                "status": "error",
                "message": f"No images found for campaign {campaign_id}. Add a seed image first."
            }
        base_metadata = json.loads(row["metadata"]) if row and row["metadata"] else {}

    # Build templated prompt with property overrides
    prompt = build_templated_prompt(base_metadata, property_overrides)
    print(f"[DEBUG generate_video_with_properties] Generated prompt: {prompt[:200]}...")

    # Generate video with custom prompt
    result = await generate_video_ad(
        campaign_id=campaign_id,
        image_id=image_id,
        custom_prompt=prompt,
        duration_seconds=duration_seconds,
        tool_context=tool_context
    )

    # Add requested properties to result
    if result["status"] == "success":
        result["requested_properties"] = property_overrides
        result["message"] = f"Video generated with custom properties: {', '.join(property_overrides.keys()) or 'defaults'}"

    return result


async def get_video_properties(ad_id: int) -> dict:
    """Get the analyzed properties for a specific video ad.

    Args:
        ad_id: The ID of the video ad

    Returns:
        Dictionary with video properties and ad details
    """
    with get_db_cursor() as cursor:
        cursor.execute('''
            SELECT ca.*, c.name as campaign_name
            FROM campaign_ads ca
            JOIN campaigns c ON ca.campaign_id = c.id
            WHERE ca.id = ?
        ''', (ad_id,))

        ad = cursor.fetchone()
        if not ad:
            return {
                "status": "error",
                "message": f"Ad with ID {ad_id} not found"
            }

        video_props = None
        if ad["video_properties"]:
            try:
                video_props = json.loads(ad["video_properties"])
            except json.JSONDecodeError:
                pass

        # If no properties exist, try to analyze the video
        if not video_props and ad["video_path"] and ad["status"] == "completed":
            video_filename = ad["video_path"]
            # Use storage abstraction for existence check
            if storage.video_exists(video_filename):
                properties = await analyze_video(video_filename)
                video_props = properties.model_dump()
                # Save to database
                cursor.execute('''
                    UPDATE campaign_ads SET video_properties = ? WHERE id = ?
                ''', (properties.model_dump_json(), ad_id))

        return {
            "status": "success",
            "ad_id": ad_id,
            "campaign_id": ad["campaign_id"],
            "campaign_name": ad["campaign_name"],
            "video_path": ad["video_path"],
            "video_properties": video_props,
            "has_properties": video_props is not None
        }


# =============================================================================
# Product and Video Listing Functions
# =============================================================================

def list_products(category: str = None, include_urls: bool = True) -> dict:
    """List all available products for video generation.

    Products are pre-loaded from the products table (22 products).
    Includes public GCS URLs for product images when available.

    Args:
        category: Optional category filter (dress, top, pants, outerwear, skirt)
        include_urls: Whether to include public image URLs (default: True)

    Returns:
        Dictionary with list of products including image URLs
    """
    from ..database.db import list_products as db_list_products

    products = db_list_products(category)

    product_list = []
    for p in products:
        product_data = {
            "id": p["id"],
            "name": p["name"],
            "category": p["category"],
            "style": p["style"],
            "color": p["color"],
            "fabric": p["fabric"],
            "image_filename": p["image_filename"]
        }

        # Add public URL for product image
        if include_urls and p["image_filename"]:
            image_url = storage.get_public_url(f"product-images/{p['image_filename']}")
            if image_url:
                product_data["image_url"] = image_url

        product_list.append(product_data)

    return {
        "status": "success",
        "product_count": len(product_list),
        "products": product_list,
        "note": "Click image_url links to view product images in browser"
    }


def list_campaign_videos(campaign_id: int = None, status: str = None) -> dict:
    """List videos from the campaign_videos table.

    This is the new function for the HITL workflow that lists videos
    with their status (generated, activated, paused, archived).

    Args:
        campaign_id: Optional campaign ID filter
        status: Optional status filter (generated, activated, paused, archived)

    Returns:
        Dictionary with list of videos and their details
    """
    with get_db_cursor() as cursor:
        # Build query based on filters
        query = '''
            SELECT cv.*, c.name as campaign_name, p.name as product_name
            FROM campaign_videos cv
            JOIN campaigns c ON cv.campaign_id = c.id
            LEFT JOIN products p ON cv.product_id = p.id
        '''
        params = []
        conditions = []

        if campaign_id:
            conditions.append("cv.campaign_id = ?")
            params.append(campaign_id)
        if status:
            conditions.append("cv.status = ?")
            params.append(status)

        if conditions:
            query += " WHERE " + " AND ".join(conditions)

        query += " ORDER BY cv.created_at DESC"

        cursor.execute(query, params)
        rows = cursor.fetchall()

        videos = []
        for row in rows:
            variation_params = None
            if row["variation_params"]:
                try:
                    variation_params = json.loads(row["variation_params"])
                except json.JSONDecodeError:
                    pass

            videos.append({
                "id": row["id"],
                "campaign_id": row["campaign_id"],
                "campaign_name": row["campaign_name"],
                "product_id": row["product_id"],
                "product_name": row["product_name"],
                "video_filename": row["video_filename"],
                "thumbnail_path": row["thumbnail_path"],
                "variation_name": row["variation_name"],
                "variation_params": variation_params,
                "pipeline_type": row["pipeline_type"],
                "duration_seconds": row["duration_seconds"],
                "status": row["status"],
                "activated_at": row["activated_at"],
                "created_at": row["created_at"],
                "generation_time_seconds": row["generation_time_seconds"]
            })

        return {
            "status": "success",
            "video_count": len(videos),
            "videos": videos
        }


async def generate_video_with_variation(
    campaign_id: int,
    product_id: int,
    variation_name: str = None,
    model_ethnicity: str = "diverse",
    setting: str = "studio",
    mood: str = "elegant",
    lighting: str = "natural",
    activity: str = "walking",
    camera_movement: str = "orbit",
    time_of_day: str = "day",
    visual_style: str = "cinematic",
    energy: str = "moderate",
    duration_seconds: int = 8,
    use_two_stage: bool = True,
    tool_context: ToolContext = None
) -> dict:
    """Generate a video with variation parameters as individual arguments.

    This is a convenience wrapper around generate_video_from_product that
    accepts variation parameters as individual arguments for easier agent usage.

    Args:
        campaign_id: The campaign to generate for
        product_id: The product ID from products table
        variation_name: Name for this variation (auto-generated if not provided)
        model_ethnicity: Model ethnicity (asian, european, african, latina, south-asian, diverse)
        setting: Setting (studio, beach, urban, cafe, rooftop, garden, nature, etc.)
        mood: Mood (elegant, romantic, bold, playful, sophisticated, etc.)
        lighting: Lighting (natural, studio, dramatic, soft, golden, neon, moody)
        activity: Activity (walking, standing, sitting, dancing, spinning, posing)
        camera_movement: Camera movement (orbit, pan, dolly, static, tracking, crane)
        time_of_day: Time of day (golden-hour, sunrise, day, sunset, dusk, night)
        visual_style: Visual style (cinematic, editorial, commercial, artistic)
        energy: Energy level (calm, moderate, dynamic, high-energy)
        duration_seconds: Video duration (4, 6, or 8 seconds)
        use_two_stage: Use two-stage pipeline (default True)
        tool_context: Optional ADK ToolContext for artifact storage

    Returns:
        Dictionary with video details
    """
    # Generate variation name if not provided
    if not variation_name:
        variation_name = f"{model_ethnicity}-{setting}-{mood}"

    # Create CreativeVariation from parameters
    variation = CreativeVariation(
        name=variation_name,
        model_ethnicity=model_ethnicity,
        setting=setting,
        mood=mood,
        lighting=lighting,
        activity=activity,
        camera_movement=camera_movement,
        time_of_day=time_of_day,
        visual_style=visual_style,
        energy=energy
    )

    # Call the main function
    return await generate_video_from_product(
        campaign_id=campaign_id,
        product_id=product_id,
        variation=variation,
        use_two_stage=use_two_stage,
        duration_seconds=duration_seconds,
        tool_context=tool_context
    )


def get_variation_presets() -> dict:
    """Get available preset variations for video generation.

    Returns predefined variation sets for:
    - diversity: Different model ethnicities
    - settings: Different locations/environments
    - moods: Different emotional tones

    Returns:
        Dictionary with preset variations
    """
    presets = {}
    for preset_name, variations in PRESET_VARIATIONS.items():
        presets[preset_name] = [
            {
                "name": v.name,
                "model_ethnicity": v.model_ethnicity,
                "setting": v.setting,
                "mood": v.mood,
                "lighting": v.lighting,
                "activity": v.activity
            }
            for v in variations
        ]

    return {
        "status": "success",
        "preset_count": len(presets),
        "presets": presets
    }
